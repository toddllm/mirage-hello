name: 🐛 Bug Report
description: File a bug report to help us improve
title: "[BUG] "
labels: ["bug"]
body:
  - type: markdown
    attributes:
      value: |
        Thanks for taking the time to file a bug report! 🐛

  - type: textarea
    id: bug-description
    attributes:
      label: Bug Description
      description: A clear description of what the bug is
      placeholder: Error occurs when running the GPU demo with batch size > 2
    validations:
      required: true

  - type: textarea
    id: environment
    attributes:
      label: Environment
      description: Your system configuration
      placeholder: |
        GPU: RTX 3090 24GB
        CUDA: 12.1
        PyTorch: 2.1.0
        Python: 3.11
        OS: Ubuntu 22.04
    validations:
      required: true

  - type: textarea
    id: reproduction-steps
    attributes:
      label: Steps to Reproduce
      description: How can we reproduce this bug?
      placeholder: |
        1. Clone the repository
        2. Run `python working_gpu_demo.py`
        3. Error occurs during heavy model benchmark
    validations:
      required: true

  - type: textarea
    id: expected-behavior
    attributes:
      label: Expected Behavior
      description: What did you expect to happen?
      placeholder: The benchmark should complete successfully without errors
    validations:
      required: true

  - type: textarea
    id: actual-behavior
    attributes:
      label: Actual Behavior
      description: What actually happened? Include error messages
      placeholder: |
        RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB...
        [Full error traceback]
    validations:
      required: true

  - type: textarea
    id: additional-context
    attributes:
      label: Additional Context
      description: Screenshots, logs, or other relevant information
      placeholder: Add any other context about the problem here